{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c05be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sample_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed737845",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[['event_type', 'event_text', 'relative_time_to_final_event']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a624497",
   "metadata": {},
   "source": [
    "### Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea4710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "print(\"Hugging Face API token configured!\")\n",
    "\n",
    "os.environ['HF_CACHE'] = '/Users/rahulbouri/Desktop/projects/mimic/hf_cache'\n",
    "print(\"Hugging Face cache configured!\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalDataPreprocessor:\n",
    "    \"\"\"\n",
    "    Handles all preprocessing steps for clinical data including:\n",
    "    - One-hot encoding of event_type\n",
    "    - ClinicalBERT embedding generation for event_text\n",
    "    - Time encoding for relative_time_to_final_event\n",
    "    - One-hot encoding of static features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert_model_name: str = \"medicalai/ClinicalBERT\", \n",
    "                 bert_embed_dim: int = 64, time_embed_dim: int = 16):\n",
    "        \"\"\"\n",
    "        Initialize preprocessor with BERT model and embedding dimensions\n",
    "        \n",
    "        Args:\n",
    "            bert_model_name: HuggingFace model name for ClinicalBERT\n",
    "            bert_embed_dim: Target dimension for BERT embeddings (reduced from 768)\n",
    "            time_embed_dim: Dimension for time encoding\n",
    "        \"\"\"\n",
    "        self.bert_model_name = bert_model_name\n",
    "        self.bert_embed_dim = bert_embed_dim\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "        \n",
    "        # Initialize BERT tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert_model = AutoModel.from_pretrained(bert_model_name)\n",
    "        self.bert_model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        # Dimensionality reduction layer for BERT embeddings\n",
    "        self.bert_reducer = nn.Linear(768, bert_embed_dim)  # ClinicalBERT outputs 768-dim\n",
    "        \n",
    "        # Initialize encoders (will be fitted during preprocessing)\n",
    "        self.event_type_encoder = None\n",
    "        self.static_encoders = {}\n",
    "        self.time_scaler = StandardScaler()\n",
    "        \n",
    "        # Store processed embeddings to avoid recomputation\n",
    "        self.precomputed_embeddings = {}\n",
    "        \n",
    "    def encode_event_type(self, event_types: pd.Series) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        One-hot encode event_type column\n",
    "        \n",
    "        Args:\n",
    "            event_types: Series containing event type strings\n",
    "            \n",
    "        Returns:\n",
    "            One-hot encoded array of shape (n_samples, n_event_types)\n",
    "        \"\"\"\n",
    "        if self.event_type_encoder is None:\n",
    "            # Fit encoder on unique event types\n",
    "            unique_types = event_types.unique()\n",
    "            self.event_type_encoder = OneHotEncoder(sparse_output=False)\n",
    "            self.event_type_encoder.fit(unique_types.reshape(-1, 1))\n",
    "            print(f\"Event types found: {unique_types}\")\n",
    "        \n",
    "        # Transform event types\n",
    "        encoded = self.event_type_encoder.transform(event_types.values.reshape(-1, 1))\n",
    "        print(f\"Event type encoding shape: {encoded.shape}\")\n",
    "        return encoded\n",
    "    \n",
    "    def encode_event_text_batch(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate ClinicalBERT embeddings for event_text in batches\n",
    "        \n",
    "        Args:\n",
    "            texts: List of clinical text descriptions\n",
    "            batch_size: Number of texts to process in each batch\n",
    "            \n",
    "        Returns:\n",
    "            Array of embeddings with shape (n_texts, bert_embed_dim)\n",
    "        \"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Process in batches to manage memory\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            \n",
    "            # Generate embeddings without gradients (frozen BERT)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert_model(**inputs)\n",
    "                # Use CLS token embedding (first token) as sentence representation\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, 768)\n",
    "                \n",
    "                # Reduce dimensionality\n",
    "                reduced_embeddings = self.bert_reducer(cls_embeddings)  # Shape: (batch_size, bert_embed_dim)\n",
    "                \n",
    "            all_embeddings.append(reduced_embeddings.numpy())\n",
    "            \n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                print(f\"Processed {min(i + batch_size, len(texts))}/{len(texts)} texts\")\n",
    "        \n",
    "        embeddings = np.vstack(all_embeddings)\n",
    "        print(f\"Generated BERT embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def encode_relative_time(self, times: pd.Series) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode relative_time_to_final_event using learned embeddings\n",
    "        \n",
    "        Args:\n",
    "            times: Series containing relative time values\n",
    "            \n",
    "        Returns:\n",
    "            Time embeddings of shape (n_samples, time_embed_dim)\n",
    "        \"\"\"\n",
    "        # Normalize time values\n",
    "        times_scaled = self.time_scaler.fit_transform(times.values.reshape(-1, 1))\n",
    "        \n",
    "        # Create time embeddings using sinusoidal encoding (similar to transformer positional encoding)\n",
    "        embeddings = np.zeros((len(times), self.time_embed_dim))\n",
    "        \n",
    "        for i in range(self.time_embed_dim):\n",
    "            if i % 2 == 0:\n",
    "                embeddings[:, i] = np.sin(times_scaled.flatten() / (10000 ** (i / self.time_embed_dim)))\n",
    "            else:\n",
    "                embeddings[:, i] = np.cos(times_scaled.flatten() / (10000 ** (i / self.time_embed_dim)))\n",
    "        \n",
    "        print(f\"Time encoding shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def encode_static_features(self, df: pd.DataFrame, static_columns: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        One-hot encode static features (ICU careunit, gender, etc.)\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing static features\n",
    "            static_columns: List of column names to encode\n",
    "            \n",
    "        Returns:\n",
    "            One-hot encoded static features\n",
    "        \"\"\"\n",
    "        all_static_encoded = []\n",
    "        \n",
    "        for col in static_columns:\n",
    "            if col not in self.static_encoders and col != 'patient_age':\n",
    "                # Handle missing values by filling with 'Unknown'\n",
    "                df[col] = df[col].fillna('Unknown')\n",
    "                \n",
    "                # Fit encoder\n",
    "                encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "                encoder.fit(df[col].values.reshape(-1, 1))\n",
    "                self.static_encoders[col] = encoder\n",
    "                \n",
    "                print(f\"Static feature '{col}' categories: {encoder.categories_[0]}\")\n",
    "            elif col not in self.static_encoders and col == 'patient_age':\n",
    "                df[col] = df[col].fillna(df[col].mean())\n",
    "                encoder = StandardScaler()\n",
    "                encoder.fit(df[col].values.reshape(-1, 1))\n",
    "                self.static_encoders[col] = encoder\n",
    "                print(f\"Static feature '{col}' categories: {encoder.categories_[0]}\")\n",
    "            \n",
    "            # Transform feature\n",
    "            encoded = self.static_encoders[col].transform(df[col].values.reshape(-1, 1))\n",
    "            all_static_encoded.append(encoded)\n",
    "        \n",
    "        if all_static_encoded:\n",
    "            static_features = np.hstack(all_static_encoded)\n",
    "            print(f\"Static features encoding shape: {static_features.shape}\")\n",
    "            return static_features\n",
    "        else:\n",
    "            return np.empty((len(df), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59865929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for clinical sequential data\n",
    "    Handles variable sequence lengths and proper batching\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sequences: Dict[str, List[np.ndarray]], \n",
    "                 static_features: np.ndarray, \n",
    "                 labels: np.ndarray,\n",
    "                 max_seq_length: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Initialize dataset\n",
    "        \n",
    "        Args:\n",
    "            sequences: Dict with keys 'event_type', 'event_text', 'time' containing lists of arrays\n",
    "            static_features: Array of static features for each patient\n",
    "            labels: Binary labels for hospital mortality\n",
    "            max_seq_length: Maximum sequence length for padding/truncation\n",
    "        \"\"\"\n",
    "        self.sequences = sequences\n",
    "        self.static_features = static_features\n",
    "        self.labels = labels\n",
    "        self.max_seq_length = max_seq_length or self._get_max_length()\n",
    "        \n",
    "    def _get_max_length(self) -> int:\n",
    "        \"\"\"Get maximum sequence length across all patients\"\"\"\n",
    "        return max(len(seq) for seq in self.sequences['event_type'])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a single sample\n",
    "        \n",
    "        Returns:\n",
    "            transformer_input: Concatenated features for transformer (seq_len, feature_dim)\n",
    "            static_features: Static features for this patient\n",
    "            label: Binary mortality label\n",
    "        \"\"\"\n",
    "        # Get sequences for this patient\n",
    "        event_type_seq = self.sequences['event_type'][idx]\n",
    "        event_text_seq = self.sequences['event_text'][idx]\n",
    "        time_seq = self.sequences['time'][idx]\n",
    "        \n",
    "        # Pad or truncate sequences\n",
    "        seq_len = min(len(event_type_seq), self.max_seq_length)\n",
    "        \n",
    "        # Concatenate features for each time step\n",
    "        transformer_input = []\n",
    "        for i in range(seq_len):\n",
    "            # Concatenate: event_type (5) + event_text (64) + time (16)\n",
    "            timestep_features = np.concatenate([\n",
    "                event_type_seq[i],      # One-hot event type\n",
    "                event_text_seq[i],      # BERT embeddings\n",
    "                time_seq[i]            # Time encoding\n",
    "            ])\n",
    "            transformer_input.append(timestep_features)\n",
    "        \n",
    "        # Pad sequence if necessary\n",
    "        while len(transformer_input) < self.max_seq_length:\n",
    "            # Zero padding\n",
    "            zero_pad = np.zeros_like(transformer_input[0])\n",
    "            transformer_input.append(zero_pad)\n",
    "        \n",
    "        transformer_input = np.array(transformer_input)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(transformer_input),\n",
    "            torch.FloatTensor(self.static_features[idx]),\n",
    "            torch.FloatTensor([self.labels[idx]])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_clinical_data(df: pd.DataFrame, \n",
    "                         patient_id_col: str = 'patient_id',\n",
    "                         test_size: float = 0.2,\n",
    "                         max_seq_length: int = 100) -> Tuple:\n",
    "    \"\"\"\n",
    "    Complete data preparation pipeline for clinical transformer model\n",
    "    \n",
    "    Args:\n",
    "        df: Raw clinical dataframe\n",
    "        patient_id_col: Column name for patient ID\n",
    "        test_size: Proportion for test split\n",
    "        max_seq_length: Maximum sequence length\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_dataset, val_dataset, test_dataset, preprocessor, model_params)\n",
    "    \"\"\"\n",
    "    print(\"Starting clinical data preparation...\")\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = ClinicalDataPreprocessor()\n",
    "    \n",
    "    # Define static feature columns\n",
    "    static_columns = [\n",
    "        'patient_gender', 'first_icu_careunit', 'last_icu_careunit',\n",
    "        'admission_type', 'admission_location', 'discharge_location',\n",
    "        'insurance', 'marital_status', 'patient_race', \n",
    "        'patient_age'\n",
    "    ]\n",
    "    \n",
    "    # Group by patient to create sequences\n",
    "    print(\"Grouping data by patient...\")\n",
    "    patient_groups = df.groupby(patient_id_col)\n",
    "    \n",
    "    # Prepare containers for processed data\n",
    "    all_sequences = {'event_type': [], 'event_text': [], 'time': []}\n",
    "    all_static_features = []\n",
    "    all_labels = []\n",
    "    patient_ids = []\n",
    "    \n",
    "    print(\"Processing patients...\")\n",
    "    for patient_id, patient_data in patient_groups:\n",
    "        if len(patient_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Sort by event time or any temporal column\n",
    "        patient_data = patient_data.sort_values('relative_time_to_final_event', ascending=False)\n",
    "        \n",
    "        # Limit sequence length\n",
    "        if len(patient_data) > max_seq_length:\n",
    "            patient_data = patient_data.head(max_seq_length)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds, preprocessor, model_params = prepare_clinical_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01494862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
